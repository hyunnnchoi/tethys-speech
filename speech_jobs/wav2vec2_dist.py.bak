import tensorflow as tf
import numpy as np
import json
import os
import sys
import time
import argparse
from transformers import TFWav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2Config
import soundfile as sf
import librosa
import requests
from io import BytesIO

parser = argparse.ArgumentParser(description='wav2vec2 Distributed Speech Recognition')
parser.add_argument('--num_batches', type=int, default=40, help='num_batches per replica, default is set 40')
parser.add_argument('--batch_size', type=int, default=1, help='batch size per replica, default is set 1')
args = parser.parse_args()

# 환경 설정
tf_config = json.loads(os.environ.get('TF_CONFIG') or '{}')
task_config = tf_config.get('task', {})
task_type = task_config.get('type')
task_index = task_config.get('index')

# 모델과 프로세서를 저장할 로컬 디렉토리 설정
CACHE_DIR = '/workspace/model_cache'  # 컨테이너 내 사전 준비된 모델 캐시 경로
DATASET_DIR = '/workspace/datasets'  # 컨테이너 내 사전 준비된 데이터셋 경로

# 분산 학습 전략 설정
strategy = tf.distribute.MultiWorkerMirroredStrategy()

# 하이퍼파라미터 설정
BATCH_SIZE_PER_REPLICA = args.batch_size
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
MAX_ITERATIONS = args.num_batches
BUFFER_SIZE = 10000

print(f'batch size per replica: {BATCH_SIZE_PER_REPLICA}, global batch size: {GLOBAL_BATCH_SIZE}')
print(f'num_batches: {MAX_ITERATIONS}')

# LibriSpeech 샘플 다운로드 함수
def download_librispeech_sample():
    """
    LibriSpeech 샘플 오디오 파일을 다운로드합니다.
    """
    print("LibriSpeech 샘플 다운로드 중...")
    
    # sample 파일을 저장할 디렉토리 생성
    os.makedirs(os.path.join(DATASET_DIR, 'librispeech_samples'), exist_ok=True)
    
    # 샘플 URL (작은 LibriSpeech 샘플 파일)
    sample_url = "https://www.openslr.org/resources/12/dev-clean.tar.gz"
    
    # 다운로드 경로
    download_path = os.path.join(DATASET_DIR, 'librispeech_samples', 'dev-clean.tar.gz')
    
    # 파일이 이미 존재하는지 확인
    if os.path.exists(download_path):
        print(f"이미 다운로드된 파일 사용: {download_path}")
        return download_path
    
    # 파일 다운로드
    print(f"파일 다운로드 중: {sample_url}")
    response = requests.get(sample_url, stream=True)
    
    if response.status_code == 200:
        with open(download_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"다운로드 완료: {download_path}")
        
        # 압축 해제
        print("압축 해제 중...")
        import tarfile
        with tarfile.open(download_path, 'r:gz') as tar:
            tar.extractall(path=os.path.join(DATASET_DIR, 'librispeech_samples'))
        print("압축 해제 완료")
        
        return download_path
    else:
        print(f"다운로드 실패: HTTP 상태 코드 {response.status_code}")
        
        # 실패 시 테스트용 더미 데이터 생성
        return create_dummy_audio_file()

def create_dummy_audio_file():
    """
    다운로드 실패 시 더미 오디오 파일을 생성합니다.
    """
    dummy_file_path = os.path.join(DATASET_DIR, 'librispeech_samples', 'dummy_audio.wav')
    os.makedirs(os.path.dirname(dummy_file_path), exist_ok=True)
    
    # 30초 길이의 더미 오디오 생성
    sr = 16000
    duration = 30  # 초
    samples = np.random.uniform(-0.1, 0.1, size=(int(duration * sr),))
    
    # WAV 파일로 저장
    sf.write(dummy_file_path, samples, sr)
    print(f"더미 오디오 파일 생성: {dummy_file_path}")
    
    return dummy_file_path

def find_audio_files(directory):
    """
    지정된 디렉토리에서 모든 .flac 오디오 파일을 찾습니다.
    """
    audio_files = []
    
    if not os.path.exists(directory):
        print(f"디렉토리가 존재하지 않음: {directory}")
        return audio_files
    
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith(".flac"):
                audio_files.append(os.path.join(root, file))
    
    print(f"발견된 오디오 파일 수: {len(audio_files)}")
    return audio_files

# 데이터셋 준비 함수
def prepare_dataset(processor, audio_files):
    """
    오디오 파일 목록에서 TensorFlow 데이터셋을 생성합니다.
    """
    features = []
    
    # 파일이 없는 경우 더미 데이터 사용
    if not audio_files:
        print("오디오 파일이 없으므로 더미 데이터 사용")
        # 충분히 긴 더미 오디오 생성 (30초)
        dummy_audio = np.random.uniform(-0.1, 0.1, size=(16000 * 30,))
        input_values = processor(dummy_audio, sampling_rate=16000, return_tensors="np").input_values[0]
        features.append(input_values)
    else:
        # 실제 오디오 파일 처리
        for audio_file in audio_files[:10]:  # 처음 10개만 사용
            try:
                # 오디오 로드
                audio, sr = librosa.load(audio_file, sr=16000)
                
                # 오디오가 너무 짧으면 패딩
                if len(audio) < 16000 * 5:  # 5초 미만이면
                    padding = np.zeros(16000 * 5 - len(audio))
                    audio = np.concatenate([audio, padding])
                
                # 프로세서로 특성 추출
                input_values = processor(audio, sampling_rate=16000, return_tensors="np").input_values[0]
                features.append(input_values)
                print(f"오디오 파일 처리 완료: {audio_file}, 길이: {len(audio)/16000:.2f}초")
            except Exception as e:
                print(f"오디오 파일 처리 중 오류: {audio_file}, {str(e)}")
    
    # 데이터셋 생성
    # 라벨은 실제로 사용되지 않으므로 더미 데이터로 설정
    dummy_labels = np.zeros((len(features), 10), dtype=np.int32)
    
    # 특성의 최대 길이 찾기
    max_length = max(len(feature) for feature in features)
    
    # 패딩 적용
    features_padded = np.array([
        np.pad(feature, (0, max_length - len(feature)), 'constant')
        for feature in features
    ])
    
    # TensorFlow 데이터셋 생성
    dataset = tf.data.Dataset.from_tensor_slices((features_padded, dummy_labels))
    
    # 배치 및 반복 설정
    return dataset.batch(GLOBAL_BATCH_SIZE).repeat()

# 직접 모델을 초기화하는 함수
def initialize_custom_wav2vec2():
    """
    기본 더미 입력 대신 커스텀 더미 입력으로 Wav2Vec2 모델을 초기화합니다.
    """
    print("모델과 프로세서 로드 중...")
    
    model_name = 'facebook/wav2vec2-base-960h'
    
    # 프로세서 로드
    processor = Wav2Vec2Processor.from_pretrained(model_name, cache_dir=CACHE_DIR)
    
    # 모델 설정 로드
    config = Wav2Vec2Config.from_pretrained(model_name, cache_dir=CACHE_DIR)
    
    # 충분히 큰 더미 입력 생성
    dummy_input = np.random.random((BATCH_SIZE_PER_REPLICA, 80000))  # 약 5초 길이의 더미 오디오
    
    # TensorFlow 모델 초기화
    model = TFWav2Vec2ForCTC(config)
    
    # 커스텀 더미 입력으로 모델 첫 호출
    _ = model(dummy_input)
    
    # 사전 훈련된 가중치 로드
    pretrained_model = TFWav2Vec2ForCTC.from_pretrained(model_name, config=config, cache_dir=CACHE_DIR)
    
    # 가중치 복사
    model.set_weights(pretrained_model.get_weights())
    
    return model, processor

# 손실 함수 정의
def compute_loss(labels, logits):
    """
    계산된 로짓과 레이블 간의 손실을 계산합니다.
    단순 MSE 손실을 사용하여 학습 과정을 안정화합니다.
    """
    # 학습 과정만 시뮬레이션하므로 간단한 손실 함수 사용
    loss = tf.reduce_mean(tf.square(logits))
    return loss

# 학습 스텝 정의
@tf.function
def train_step(inputs, labels):
    with tf.GradientTape() as tape:
        outputs = model(inputs, training=True)
        loss = compute_loss(labels, outputs.logits)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

# 분산 학습 스텝 정의
@tf.function
def distributed_train_step(inputs, labels):
    per_replica_losses = strategy.run(train_step, args=(inputs, labels))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

# 메인 실행 함수
def main():
    global model, optimizer
    
    print("Speech 분산 학습 시작...")
    
    # LibriSpeech 샘플 데이터 다운로드
    download_librispeech_sample()
    
    # 오디오 파일 검색
    audio_files = find_audio_files(os.path.join(DATASET_DIR, 'librispeech_samples', 'LibriSpeech'))
    
    with strategy.scope():
        # 모델과 프로세서 초기화
        model, processor = initialize_custom_wav2vec2()
        
        # 옵티마이저 설정
        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
    
    # 데이터셋 준비
    train_dataset = prepare_dataset(processor, audio_files)
    dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
    
    # 학습 실행
    os.system('sh /workspace/network.sh &') # network profile
    os.system('sh /workspace/gpu.sh &') # gpu profile
    print('''
========================
network profile started!
========================''')
    # jct
    start_time = time.time()
    start_time_tf = tf.timestamp()

    iterator = iter(dist_dataset)
    total_loss = tf.constant(0.0)
    
    # 학습 루프
    for iteration in tf.range(MAX_ITERATIONS):
        inputs, labels = next(iterator)
        loss = distributed_train_step(inputs, labels)
        total_loss += loss

        tf.print("Timestamp: ", tf.timestamp() - start_time_tf, "Step:", iteration, "Loss:", loss, output_stream=sys.stdout)

    average_loss = total_loss / tf.cast(MAX_ITERATIONS, tf.float32)

    # jct
    end_time = time.time()
    jct = end_time - start_time

    # average_loss 텐서를 평가하여 출력
    tf.print("Training completed. Average Loss:", average_loss, output_stream=sys.stdout)
    print("jct:", jct)
    
    # jct in tf_cnn_benchmark = wall_time in perfzero
    model_txt = open('/workspace/model.txt','r')
    save_dir_name = model_txt.read()
    jct_file = open('/result/' + save_dir_name.strip() + '/' + task_type + '_' + str(task_index) + '_jct.txt', 'w')
    jct_file.write('%.2f' % (float(jct)))
    jct_file.close()
    model_txt.close()

if __name__ == "__main__":
    main()